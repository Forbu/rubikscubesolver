{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chose the current file directory as the working directory\n",
    "import os\n",
    "os.chdir(\"/teamspace/studios/this_studio/rubikscubesolver\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cuda_plugin_extension is not found.\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "import wandb  # for logging\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax.nnx as nnx\n",
    "\n",
    "import rubiktransformer.dataset as dataset\n",
    "from rubiktransformer.trainer import reshape_sample\n",
    "\n",
    "from rubiktransformer.trainer_online import init_model_optimizer, init_buffer, train_step_transformer_rf, training_loop\n",
    "from rubiktransformer.online_training_utils import run_n_steps, reshape_diffusion_setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mforbu14\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.18.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/teamspace/studios/this_studio/rubikscubesolver/wandb/run-20240914_132509-03ke1l0k</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/forbu14/RubikTransformer/runs/03ke1l0k' target=\"_blank\">experiment_20240914-132508</a></strong> to <a href='https://wandb.ai/forbu14/RubikTransformer' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/forbu14/RubikTransformer' target=\"_blank\">https://wandb.ai/forbu14/RubikTransformer</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/forbu14/RubikTransformer/runs/03ke1l0k' target=\"_blank\">https://wandb.ai/forbu14/RubikTransformer/runs/03ke1l0k</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/forbu14/RubikTransformer/runs/03ke1l0k?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7fc4ac3dc160>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    \"\"\"Configuration class\"\"\"\n",
    "\n",
    "    jax_key: jnp.ndarray = jax.random.PRNGKey(49)\n",
    "    rngs = nnx.Rngs(48)\n",
    "    batch_size: int = 128\n",
    "    lr_1: float = 4e-4\n",
    "    lr_2: float = 4e-4\n",
    "    nb_games: int = 128 * 100\n",
    "    len_seq: int = 32\n",
    "    nb_step: int = 1000000\n",
    "    max_length_buffer: int = 1024 * 10\n",
    "    log_every_step: int = 10\n",
    "    log_eval_every_step: int = 10\n",
    "    log_policy_reward_every_step: int = 10\n",
    "    add_data_every_step: int = 500\n",
    "\n",
    "    save_model_every_step: int = 2000\n",
    "\n",
    "\n",
    "config = Config()\n",
    "\n",
    "# init wandb config\n",
    "user = \"forbu14\"\n",
    "project = \"RubikTransformer\"\n",
    "display_name = \"experiment_\" + time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "wandb.init(entity=user, project=project, name=display_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "(\n",
    "    optimizer_diffuser,\n",
    "    optimizer_inverse,\n",
    "    metrics_train,\n",
    "    metrics_eval,\n",
    "    metrics_inverse,\n",
    "    transformer,\n",
    "    inverse_rl_model,\n",
    ") = init_model_optimizer(config)\n",
    "\n",
    "env, buffer, buffer_eval, buffer_list, buffer_list_eval, jit_step = init_buffer(\n",
    "    config\n",
    ")\n",
    "\n",
    "vmap_reset = jax.vmap(jax.jit(env.reset))\n",
    "vmap_step = jax.vmap(run_n_steps, in_axes=(0, 0, None))\n",
    "\n",
    "##### TRAINING #####\n",
    "key, subkey = jax.random.split(config.jax_key)\n",
    "config.jax_key = key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "buffer, buffer_list = dataset.fast_gathering_data_diffusion(\n",
    "    env,\n",
    "    vmap_reset,\n",
    "    vmap_step,\n",
    "    config.nb_games * 1,  # old is int(config.nb_games * 10.0),\n",
    "    config.len_seq,\n",
    "    buffer,\n",
    "    buffer_list,\n",
    "    subkey,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load weight from world model transformer:\n",
    "import pickle\n",
    "\n",
    "filename = \"state_ddt_model_improved_v2.pickle\"\n",
    "\n",
    "with open(filename, \"rb\") as input_file:\n",
    "    state = pickle.load(input_file)\n",
    "\n",
    "nnx.update(transformer, state)\n",
    "\n",
    "# load weight from world model transformer:\n",
    "import pickle\n",
    "\n",
    "filename = \"state_inverse_rl_model_improved_v2.pickle\"\n",
    "\n",
    "with open(filename, \"rb\") as input_file:\n",
    "    state = pickle.load(input_file)\n",
    "\n",
    "nnx.update(inverse_rl_model, state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = buffer.sample(buffer_list, subkey)\n",
    "sample = reshape_diffusion_setup(sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['action', 'reward', 'state_histo', 'time_step', 'context', 'state_past', 'state_future', 'state_future_noise', 'action_inverse', 'state_histo_inverse_t', 'state_histo_inverse_td1'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[0., 0., 1., ..., 1., 0., 0.],\n",
       "       [0., 1., 0., ..., 1., 0., 0.],\n",
       "       [0., 0., 1., ..., 1., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 1., 0., 0.],\n",
       "       [0., 0., 1., ..., 1., 0., 0.],\n",
       "       [0., 1., 0., ..., 1., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample[\"action_inverse\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[0., 0., 0., ..., 1., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.],\n",
       "       ...,\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample[\"state_histo_inverse_t\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[0., 0., 0., ..., 0., 1., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 1., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample[\"state_histo_inverse_td1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[-58.14122  , -12.908836 , 107.829445 , -26.21095  , -87.36493  ,\n",
       "        -19.743734 ],\n",
       "       [-17.515263 ,  80.81346  , -28.577995 , -49.040886 , -30.555523 ,\n",
       "        -17.010876 ],\n",
       "       [-30.758236 , -30.516747 ,  76.49304  , -14.1797085, -48.68231  ,\n",
       "        -10.990468 ],\n",
       "       ...,\n",
       "       [-23.25964  , -62.139297 , -24.637434 ,  67.35371  , -12.765332 ,\n",
       "        -13.466367 ],\n",
       "       [-33.95061  , -17.303339 ,  73.70761  , -25.77107  , -45.953106 ,\n",
       "        -11.735286 ],\n",
       "       [-15.208434 ,  78.16751  , -26.444832 , -57.60352  , -29.599823 ,\n",
       "        -18.02725  ]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inverse_rl_model(sample[\"state_histo_inverse_t\"], sample[\"state_histo_inverse_td1\"])[:, :6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sampling_model(key, model, sample_eval, nb_step=100, config=None, target_reward=0.5):\n",
    "    \"\"\"\n",
    "    Function used to sampling a state from a list \n",
    "    \"\"\"\n",
    "    seq_len_future = config.len_seq - config.len_seq // 4 \n",
    "    noise_future  = jax.random.dirichlet(key, jnp.ones(6) * 5., (config.batch_size, seq_len_future, 54))\n",
    "    sample_eval[\"reward\"] = jnp.linspace(start=target_reward, stop=0.1 + target_reward, num=config.batch_size)[:, None]\n",
    "\n",
    "    for t_step in range(nb_step):\n",
    "        t_step_array = jnp.ones((config.batch_size, 1, 1, 1)) * float(t_step / nb_step)\n",
    "        sample_eval[\"context\"] = jnp.concatenate([sample_eval[\"reward\"], t_step_array[:, :, 0, 0]], axis=1)\n",
    "\n",
    "        estimation_logits_past, estimation_logits_future = model(\n",
    "            sample_eval[\"state_past\"], noise_future, sample_eval[\"context\"]\n",
    "        )\n",
    "\n",
    "        estimation_proba_future = jax.nn.softmax(estimation_logits_future, axis=-1)\n",
    "\n",
    "        noise_future = noise_future + float(1. / nb_step) * 1./ (1. - t_step_array + 0.0001) * (estimation_proba_future - noise_future)\n",
    "\n",
    "    return noise_future\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "key, subkey = jax.random.split(config.jax_key)\n",
    "config.jax_key = key\n",
    "\n",
    "buffer_eval, buffer_list_eval = dataset.fast_gathering_data_diffusion(\n",
    "    env,\n",
    "    vmap_reset,\n",
    "    vmap_step,\n",
    "    int(config.batch_size),\n",
    "    config.len_seq,\n",
    "    buffer_eval,\n",
    "    buffer_list_eval,\n",
    "    subkey,\n",
    ")\n",
    "\n",
    "sample = buffer_eval.sample(buffer_list_eval, subkey)\n",
    "sample = reshape_diffusion_setup(sample, subkey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[[[1.19809993e-05, 3.65849119e-05, 9.99903500e-01,\n",
       "          1.72968721e-05, 1.96222682e-05, 1.11013651e-05],\n",
       "         [1.23428181e-05, 1.35994051e-05, 9.99916852e-01,\n",
       "          1.42016215e-05, 2.73603946e-05, 1.56546012e-05],\n",
       "         [1.42597128e-05, 1.47442333e-05, 1.83022348e-05,\n",
       "          9.99920487e-01, 1.46263046e-05, 1.76462345e-05],\n",
       "         ...,\n",
       "         [9.99912381e-01, 1.26255909e-05, 1.34599395e-05,\n",
       "          1.71344727e-05, 1.53474975e-05, 2.90414318e-05],\n",
       "         [1.77901238e-05, 1.84202800e-05, 1.11357076e-05,\n",
       "          9.99912977e-01, 2.40276568e-05, 1.56586757e-05],\n",
       "         [9.99908090e-01, 2.08260026e-05, 3.67206521e-05,\n",
       "          1.23797217e-05, 9.59716272e-06, 1.24993967e-05]],\n",
       "\n",
       "        [[9.71351983e-06, 2.58991495e-05, 9.99895155e-01,\n",
       "          1.53561123e-05, 3.16093210e-05, 2.23021489e-05],\n",
       "         [8.55122926e-06, 3.54822259e-06, 9.99925613e-01,\n",
       "          3.59665137e-05, 1.46027887e-05, 1.17224408e-05],\n",
       "         [1.04872743e-05, 3.10603064e-05, 2.03154050e-05,\n",
       "          9.99902308e-01, 2.40781810e-05, 1.16728479e-05],\n",
       "         ...,\n",
       "         [9.99907017e-01, 1.10889086e-05, 1.91935105e-05,\n",
       "          2.98579689e-05, 1.61242206e-05, 1.67002436e-05],\n",
       "         [1.71642751e-05, 1.43517973e-05, 2.07242556e-05,\n",
       "          9.99914467e-01, 2.21568625e-05, 1.11386180e-05],\n",
       "         [9.99918401e-01, 1.17756426e-05, 1.14521245e-05,\n",
       "          2.10721046e-05, 2.14478932e-05, 1.59123447e-05]],\n",
       "\n",
       "        [[3.89863271e-05, 2.02462543e-05, 9.99907613e-01,\n",
       "          1.57405157e-05, 1.39506301e-05, 3.47790774e-06],\n",
       "         [1.25473598e-05, 2.73522455e-05, 9.99913692e-01,\n",
       "          1.61359785e-05, 1.54571608e-05, 1.48183899e-05],\n",
       "         [2.42278911e-05, 3.29478644e-06, 1.77971087e-05,\n",
       "          9.99907672e-01, 3.24591529e-05, 1.45534286e-05],\n",
       "         ...,\n",
       "         [9.99900997e-01, 7.31338514e-06, 3.82135622e-05,\n",
       "          1.48199033e-05, 2.06113327e-05, 1.79947820e-05],\n",
       "         [2.11673323e-05, 2.55552586e-05, 1.18149910e-05,\n",
       "          9.99917269e-01, 3.96140967e-06, 2.02283263e-05],\n",
       "         [9.99914110e-01, 2.92113982e-05, 1.17593445e-05,\n",
       "          1.69408740e-05, 1.51579734e-05, 1.28719257e-05]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[1.92794250e-05, 9.99904692e-01, 2.28539575e-05,\n",
       "          2.20581423e-05, 1.44980149e-05, 1.65672973e-05],\n",
       "         [3.31160962e-04, 6.20409846e-05, 1.61685375e-03,\n",
       "          9.97629106e-01, 3.27142305e-04, 3.37171368e-05],\n",
       "         [9.99904156e-01, 1.81336654e-05, 2.87697185e-05,\n",
       "          8.34075036e-06, 7.38003291e-06, 3.31683550e-05],\n",
       "         ...,\n",
       "         [1.85869867e-05, 9.99888957e-01, 2.43245158e-05,\n",
       "          2.02883966e-05, 1.83576485e-05, 2.95562204e-05],\n",
       "         [3.15534417e-05, 1.72164291e-05, 4.48853243e-05,\n",
       "          2.04716693e-04, 9.99675572e-01, 2.59662047e-05],\n",
       "         [1.29841501e-05, 1.65640377e-05, 9.99896467e-01,\n",
       "          2.83375848e-05, 2.49473378e-05, 2.07454432e-05]],\n",
       "\n",
       "        [[3.62056307e-05, 2.09087739e-05, 1.02759805e-05,\n",
       "          2.24444084e-05, 3.50452028e-05, 9.99875069e-01],\n",
       "         [2.09370628e-05, 2.54148617e-05, 2.47566495e-05,\n",
       "          9.99860764e-01, 4.02629375e-05, 2.80039385e-05],\n",
       "         [9.99902666e-01, 1.21867051e-05, 2.02716328e-05,\n",
       "          1.86827965e-05, 3.10067553e-05, 1.53107103e-05],\n",
       "         ...,\n",
       "         [9.99643147e-01, 2.14605883e-04, 3.25127039e-05,\n",
       "          3.36768571e-05, 4.04401217e-05, 3.55583616e-05],\n",
       "         [2.13671010e-05, 2.66323332e-05, 1.64562371e-05,\n",
       "          1.86531106e-05, 9.99907970e-01, 8.94558616e-06],\n",
       "         [2.67662108e-05, 1.68099068e-05, 9.99233186e-01,\n",
       "          1.41817378e-04, 5.28981909e-04, 5.24912030e-05]],\n",
       "\n",
       "        [[2.47691059e-05, 1.30939530e-04, 7.21740071e-06,\n",
       "          9.99781549e-01, 3.42525309e-05, 2.12171581e-05],\n",
       "         [1.13263959e-05, 1.68189872e-05, 1.78740593e-05,\n",
       "          9.99893188e-01, 3.12381890e-05, 2.96575017e-05],\n",
       "         [9.99926031e-01, 2.28190329e-05, 2.61748210e-05,\n",
       "          1.10659748e-05, 9.00326995e-06, 4.97419387e-06],\n",
       "         ...,\n",
       "         [1.60707859e-05, 9.99696910e-01, 1.19217439e-05,\n",
       "          1.83413504e-05, 2.24940479e-04, 3.18377279e-05],\n",
       "         [8.36048275e-06, 9.72527778e-06, 2.29058787e-05,\n",
       "          3.55197117e-05, 9.99904692e-01, 1.88117847e-05],\n",
       "         [9.36689321e-05, 1.94803579e-05, 9.97285008e-01,\n",
       "          3.23189888e-05, 2.49575917e-03, 7.38194212e-05]]],\n",
       "\n",
       "\n",
       "       [[[1.54054724e-05, 9.99904275e-01, 3.35955992e-05,\n",
       "          2.36106571e-05, 9.94233415e-06, 1.33108115e-05],\n",
       "         [1.74652087e-05, 9.61159822e-06, 1.67401740e-05,\n",
       "          2.25917902e-05, 9.99918103e-01, 1.55135058e-05],\n",
       "         [1.87174883e-05, 9.99928057e-01, 4.28184285e-06,\n",
       "          2.41058879e-05, 8.73039244e-06, 1.61225908e-05],\n",
       "         ...,\n",
       "         [2.84549315e-05, 9.99910533e-01, 1.41283963e-05,\n",
       "          2.96074431e-05, 6.14591409e-06, 1.11694681e-05],\n",
       "         [1.53995352e-05, 2.26933043e-05, 9.99920189e-01,\n",
       "          3.52913048e-06, 1.61076896e-05, 2.20816582e-05],\n",
       "         [9.49105015e-06, 9.05088382e-06, 7.74278305e-06,\n",
       "          9.99924660e-01, 3.05026770e-05, 1.85545068e-05]],\n",
       "\n",
       "        [[3.18046659e-05, 9.99900043e-01, 2.53308099e-05,\n",
       "          1.06936786e-05, 1.91346044e-05, 1.29420077e-05],\n",
       "         [1.92085281e-05, 1.79171329e-05, 2.41233502e-05,\n",
       "          1.18163880e-05, 9.99911726e-01, 1.52556458e-05],\n",
       "         [2.24509276e-05, 1.95172615e-05, 1.74042070e-05,\n",
       "          5.93875302e-06, 9.99912441e-01, 2.22770032e-05],\n",
       "         ...,\n",
       "         [1.23630743e-05, 9.99916136e-01, 1.04067149e-05,\n",
       "          2.03601085e-05, 1.61444768e-05, 2.46020500e-05],\n",
       "         [1.75610185e-05, 1.03252241e-05, 9.99909699e-01,\n",
       "          2.37687491e-05, 1.58337643e-05, 2.27349810e-05],\n",
       "         [1.43260695e-05, 1.47795072e-05, 2.45056581e-05,\n",
       "          1.88306440e-05, 9.99913812e-01, 1.38011528e-05]],\n",
       "\n",
       "        [[1.54072186e-05, 9.99917746e-01, 1.26389787e-05,\n",
       "          1.10542169e-05, 3.14419158e-05, 1.17080053e-05],\n",
       "         [2.54774932e-05, 7.63492426e-06, 2.08183192e-05,\n",
       "          1.79365743e-05, 9.99894559e-01, 3.36079393e-05],\n",
       "         [4.76004789e-06, 9.99911249e-01, 2.64260452e-05,\n",
       "          2.91492324e-05, 1.65420352e-05, 1.19158067e-05],\n",
       "         ...,\n",
       "         [1.88627746e-05, 9.99906957e-01, 1.15865842e-05,\n",
       "          1.32101122e-05, 2.16607004e-05, 2.77273357e-05],\n",
       "         [1.86486868e-05, 7.72258500e-06, 9.99913514e-01,\n",
       "          1.09832035e-05, 2.18332279e-05, 2.73517799e-05],\n",
       "         [2.24469695e-05, 1.35729788e-05, 2.15079635e-05,\n",
       "          9.99904990e-01, 2.37086788e-05, 1.37964962e-05]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[2.16327608e-05, 2.18888745e-05, 1.53022120e-05,\n",
       "          1.70986168e-05, 9.99906600e-01, 1.75466994e-05],\n",
       "         [9.99768555e-01, 1.46654202e-05, 1.54476613e-04,\n",
       "          2.29370780e-05, 2.36879569e-05, 1.57154864e-05],\n",
       "         [4.23849560e-05, 1.34974252e-05, 1.36147719e-05,\n",
       "          9.99899149e-01, 2.30928417e-05, 8.22410220e-06],\n",
       "         ...,\n",
       "         [1.09398970e-05, 8.63161404e-06, 3.29005998e-05,\n",
       "          1.58858020e-05, 2.68567819e-05, 9.99904811e-01],\n",
       "         [2.08863057e-05, 9.99557197e-01, 2.32462771e-05,\n",
       "          1.59156043e-05, 3.59207392e-04, 2.35568732e-05],\n",
       "         [9.99691963e-01, 2.56136991e-05, 9.50082904e-06,\n",
       "          1.37636671e-05, 2.27889977e-05, 2.36362219e-04]],\n",
       "\n",
       "        [[1.37523748e-05, 9.99898672e-01, 2.62141693e-05,\n",
       "          3.24547291e-05, 1.33332796e-05, 1.55813759e-05],\n",
       "         [1.13355108e-01, 2.54048500e-05, 2.23405659e-05,\n",
       "          1.67405233e-04, 1.72284199e-05, 8.86412561e-01],\n",
       "         [9.82650090e-06, 3.31001356e-05, 8.77951970e-06,\n",
       "          9.99912620e-01, 2.42963433e-05, 1.13847200e-05],\n",
       "         ...,\n",
       "         [1.85789540e-05, 9.99333084e-01, 2.38196459e-04,\n",
       "          3.42045445e-04, 1.33303693e-05, 5.47221862e-05],\n",
       "         [1.27858948e-05, 9.96478915e-01, 5.36357984e-05,\n",
       "          1.65818492e-05, 3.41543555e-03, 2.26672273e-05],\n",
       "         [2.12736242e-02, 2.16020271e-05, 9.78652179e-01,\n",
       "          2.51284800e-05, 1.64427329e-05, 1.10229594e-05]],\n",
       "\n",
       "        [[4.28729691e-05, 2.42416281e-05, 7.71362102e-06,\n",
       "          9.99883354e-01, 1.77003676e-05, 2.41267262e-05],\n",
       "         [5.97073231e-05, 4.88754595e-05, 9.99014616e-01,\n",
       "          3.98779375e-04, 2.96758953e-05, 4.48304228e-04],\n",
       "         [2.86148861e-05, 1.92624284e-05, 9.54052666e-06,\n",
       "          9.99897659e-01, 2.27647834e-05, 2.21743248e-05],\n",
       "         ...,\n",
       "         [2.77140643e-05, 9.99885082e-01, 1.89362327e-05,\n",
       "          2.56086932e-05, 1.57938339e-05, 2.68770382e-05],\n",
       "         [3.18153761e-05, 1.67072518e-04, 2.24977499e-04,\n",
       "          3.09457537e-05, 9.99487400e-01, 5.77484025e-05],\n",
       "         [1.99284405e-05, 1.36232702e-05, 9.99910355e-01,\n",
       "          1.48882391e-05, 1.70974527e-05, 2.42954120e-05]]],\n",
       "\n",
       "\n",
       "       [[[2.39494257e-05, 1.80457719e-05, 1.82196964e-05,\n",
       "          9.08505172e-06, 9.99902666e-01, 2.80814711e-05],\n",
       "         [1.30622648e-05, 3.04926652e-05, 1.74550805e-05,\n",
       "          1.80994393e-05, 4.99363523e-06, 9.99915898e-01],\n",
       "         [9.99902368e-01, 4.15658578e-05, 1.52029097e-05,\n",
       "          1.05077634e-05, 1.46621605e-05, 1.57685718e-05],\n",
       "         ...,\n",
       "         [1.45813683e-05, 2.76768114e-05, 1.34133734e-05,\n",
       "          9.99917209e-01, 8.48650234e-06, 1.87131809e-05],\n",
       "         [9.99889851e-01, 3.01019754e-05, 5.90819400e-06,\n",
       "          4.43733297e-05, 2.23091338e-05, 7.44691351e-06],\n",
       "         [9.99913931e-01, 1.54551817e-05, 1.08910026e-05,\n",
       "          1.79850031e-05, 1.12948474e-05, 3.04190908e-05]],\n",
       "\n",
       "        [[9.99919474e-01, 1.73394801e-05, 1.76455360e-05,\n",
       "          1.78854680e-05, 1.37044117e-05, 1.39831100e-05],\n",
       "         [2.19605863e-05, 9.99899268e-01, 1.46906823e-05,\n",
       "          1.18269818e-05, 1.86742982e-05, 3.36107332e-05],\n",
       "         [2.51138117e-05, 1.73060689e-05, 9.99899864e-01,\n",
       "          1.21743651e-05, 2.37652566e-05, 2.17903871e-05],\n",
       "         ...,\n",
       "         [1.67060643e-05, 7.19877426e-06, 8.94354889e-06,\n",
       "          9.99927402e-01, 2.34397594e-05, 1.63012883e-05],\n",
       "         [9.99921501e-01, 1.53455185e-05, 9.33418050e-06,\n",
       "          2.01734947e-05, 2.17498746e-05, 1.20187178e-05],\n",
       "         [9.99909043e-01, 2.56353524e-05, 1.20541081e-05,\n",
       "          1.88025879e-05, 1.66301616e-05, 1.78478658e-05]],\n",
       "\n",
       "        [[9.99903202e-01, 1.32215209e-05, 3.29893082e-05,\n",
       "          1.96530018e-05, 1.64743979e-05, 1.44249061e-05],\n",
       "         [8.30111094e-06, 9.99905169e-01, 2.11647712e-05,\n",
       "          2.13342719e-05, 1.52998837e-05, 2.87259463e-05],\n",
       "         [1.23095233e-05, 9.99901474e-01, 1.81238865e-05,\n",
       "          1.39127951e-05, 2.80863605e-05, 2.61224341e-05],\n",
       "         ...,\n",
       "         [9.69599932e-06, 1.09456014e-05, 3.23986169e-05,\n",
       "          9.99906719e-01, 1.93434535e-05, 2.08802521e-05],\n",
       "         [9.99876022e-01, 2.10956205e-05, 1.66083919e-05,\n",
       "          4.71096719e-05, 2.00995710e-05, 1.91167928e-05],\n",
       "         [1.83217926e-05, 9.99902904e-01, 2.42998358e-05,\n",
       "          1.45167578e-05, 1.90752326e-05, 2.08814163e-05]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[9.99908745e-01, 7.38084782e-06, 2.32346356e-05,\n",
       "          2.58637592e-05, 1.42292120e-05, 2.05377582e-05],\n",
       "         [9.99906659e-01, 2.14199536e-05, 2.03941017e-05,\n",
       "          1.77167822e-05, 1.15032308e-05, 2.22977251e-05],\n",
       "         [2.25091353e-05, 1.57051254e-05, 2.11130828e-05,\n",
       "          9.99906421e-01, 2.39014626e-05, 1.03244092e-05],\n",
       "         ...,\n",
       "         [1.58656621e-05, 1.97116751e-05, 2.48062424e-05,\n",
       "          2.15698965e-05, 9.99898136e-01, 1.99009664e-05],\n",
       "         [9.99695539e-01, 2.92670447e-05, 8.36624531e-06,\n",
       "          2.04386190e-04, 9.80839832e-06, 5.26811928e-05],\n",
       "         [2.86565628e-05, 2.94221099e-05, 2.27123965e-05,\n",
       "          4.15288378e-06, 9.99900937e-01, 1.41366618e-05]],\n",
       "\n",
       "        [[9.99896109e-01, 2.66910065e-05, 3.40174884e-05,\n",
       "          2.05792021e-05, 8.98930011e-06, 1.35756563e-05],\n",
       "         [4.53700777e-05, 1.84344826e-05, 2.34625768e-05,\n",
       "          1.82152458e-03, 4.64930199e-05, 9.98044729e-01],\n",
       "         [9.99686241e-01, 1.42258010e-04, 6.76445197e-05,\n",
       "          5.51464036e-05, 1.16558513e-05, 3.69796762e-05],\n",
       "         ...,\n",
       "         [1.30670378e-05, 1.70938438e-05, 2.39682849e-05,\n",
       "          1.94211025e-05, 9.99718010e-01, 2.08480284e-04],\n",
       "         [3.85969058e-02, 9.91237815e-04, 1.99521892e-04,\n",
       "          9.22945738e-01, 3.41362990e-02, 3.13025597e-03],\n",
       "         [1.73832523e-05, 1.62617071e-05, 1.80552015e-05,\n",
       "          6.75587216e-06, 9.99904096e-01, 3.74715310e-05]],\n",
       "\n",
       "        [[9.99900699e-01, 2.54875049e-05, 2.07230914e-05,\n",
       "          2.02574302e-05, 1.07204542e-05, 2.21470837e-05],\n",
       "         [1.91134168e-05, 2.22946983e-05, 1.45450467e-05,\n",
       "          1.69888372e-05, 1.95822213e-05, 9.99907494e-01],\n",
       "         [4.62166499e-05, 2.39859801e-05, 3.02437693e-05,\n",
       "          9.99864757e-01, 1.21466583e-05, 2.25952826e-05],\n",
       "         ...,\n",
       "         [2.20278744e-05, 9.53540439e-06, 1.28549291e-05,\n",
       "          3.63017898e-06, 9.99322653e-01, 6.29343092e-04],\n",
       "         [1.12696656e-03, 1.38862885e-03, 3.09890602e-05,\n",
       "          9.79882538e-01, 9.04866029e-04, 1.66661199e-02],\n",
       "         [3.50200571e-05, 3.78843397e-05, 3.41910636e-05,\n",
       "          2.34348641e-04, 2.61500431e-03, 9.97043490e-01]]],\n",
       "\n",
       "\n",
       "       ...,\n",
       "\n",
       "\n",
       "       [[[3.41401901e-05, 9.99905705e-01, 1.36923045e-05,\n",
       "          1.67757971e-05, 1.85343670e-05, 1.12630660e-05],\n",
       "         [1.61678763e-05, 1.88444974e-05, 1.44898659e-05,\n",
       "          9.99929905e-01, 9.13965050e-06, 1.14645809e-05],\n",
       "         [1.53593719e-05, 1.13989227e-05, 1.85152749e-05,\n",
       "          9.99902904e-01, 3.03634442e-05, 2.14853790e-05],\n",
       "         ...,\n",
       "         [9.99907196e-01, 2.26346310e-05, 1.55998860e-05,\n",
       "          1.90418214e-05, 1.71430875e-05, 1.82946678e-05],\n",
       "         [1.43512152e-05, 9.73453280e-06, 9.99915242e-01,\n",
       "          2.36537308e-05, 1.09531684e-05, 2.60423403e-05],\n",
       "         [1.72571745e-05, 2.49326695e-05, 1.00309262e-05,\n",
       "          9.99904811e-01, 1.87092228e-05, 2.42958777e-05]],\n",
       "\n",
       "        [[1.05454819e-05, 9.99900877e-01, 1.68648548e-05,\n",
       "          2.22579110e-05, 3.58559191e-05, 1.36505114e-05],\n",
       "         [3.07385344e-05, 3.51719791e-06, 2.05019023e-05,\n",
       "          9.99918461e-01, 1.23017235e-05, 1.44903315e-05],\n",
       "         [1.95817556e-05, 2.15948094e-05, 3.57849058e-05,\n",
       "          9.99892712e-01, 1.11445552e-05, 1.92095758e-05],\n",
       "         ...,\n",
       "         [9.99921262e-01, 1.70284184e-05, 1.44112855e-05,\n",
       "          1.22528290e-05, 1.99419446e-05, 1.51348067e-05],\n",
       "         [1.81209762e-05, 1.38308387e-05, 9.99913692e-01,\n",
       "          4.58717113e-06, 2.13205349e-05, 2.84416601e-05],\n",
       "         [2.37186905e-05, 1.49059342e-05, 1.36162853e-05,\n",
       "          9.99915183e-01, 1.79379713e-05, 1.47419050e-05]],\n",
       "\n",
       "        [[2.57815700e-05, 9.99898434e-01, 2.08725687e-05,\n",
       "          2.22856179e-05, 1.41412020e-05, 1.85118988e-05],\n",
       "         [1.80468196e-05, 1.32310670e-05, 2.88030133e-05,\n",
       "          9.99906182e-01, 9.65245999e-06, 2.41196249e-05],\n",
       "         [2.34141480e-05, 3.16970982e-05, 9.05839261e-06,\n",
       "          9.99903738e-01, 7.52985943e-06, 2.46355776e-05],\n",
       "         ...,\n",
       "         [9.99919534e-01, 2.13985331e-05, 1.46313105e-05,\n",
       "          1.10769179e-05, 8.35990068e-06, 2.50530429e-05],\n",
       "         [1.44565711e-05, 1.69347040e-05, 9.99911070e-01,\n",
       "          1.93027081e-05, 1.11071859e-05, 2.71415338e-05],\n",
       "         [9.26567009e-06, 2.35545449e-05, 1.78606715e-05,\n",
       "          9.99905050e-01, 1.74024608e-05, 2.68905424e-05]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[3.13024502e-05, 9.22195613e-06, 2.28690915e-05,\n",
       "          9.99896288e-01, 1.44678634e-05, 2.57580541e-05],\n",
       "         [2.33538449e-05, 2.35699117e-05, 9.99867558e-01,\n",
       "          4.47744969e-05, 2.57124193e-05, 1.50438864e-05],\n",
       "         [2.51976307e-05, 4.21403674e-05, 9.99858081e-01,\n",
       "          1.94904860e-05, 2.71594618e-05, 2.80661043e-05],\n",
       "         ...,\n",
       "         [2.62260437e-05, 2.50075245e-05, 2.49440782e-05,\n",
       "          2.59030494e-05, 9.99836743e-01, 6.12286385e-05],\n",
       "         [3.22018750e-05, 1.21778576e-05, 9.99890983e-01,\n",
       "          9.08767106e-06, 3.65599990e-05, 1.90422870e-05],\n",
       "         [2.88274605e-05, 1.17839081e-05, 9.99723911e-01,\n",
       "          1.47277024e-04, 3.32437921e-05, 5.49468677e-05]],\n",
       "\n",
       "        [[4.18743026e-03, 7.92376231e-05, 7.83097930e-04,\n",
       "          1.48338079e-03, 2.48670112e-05, 9.93441939e-01],\n",
       "         [8.57834458e-01, 3.06151807e-03, 1.17413238e-01,\n",
       "          9.52803437e-03, 8.67437758e-03, 3.48830177e-03],\n",
       "         [9.99623716e-01, 1.43611105e-04, 1.27179082e-04,\n",
       "          4.83073527e-05, 3.64618609e-05, 2.06711702e-05],\n",
       "         ...,\n",
       "         [1.02899503e-05, 3.08571383e-04, 1.07788946e-05,\n",
       "          2.93583144e-05, 9.99621451e-01, 1.96170295e-05],\n",
       "         [1.06871594e-05, 5.88875264e-06, 9.99909341e-01,\n",
       "          3.83597799e-05, 1.70387793e-05, 1.87049154e-05],\n",
       "         [4.69106948e-04, 1.64149096e-05, 4.17539850e-05,\n",
       "          9.99406278e-01, 3.23224813e-05, 3.41822160e-05]],\n",
       "\n",
       "        [[9.95900393e-01, 1.83529919e-05, 6.76957425e-05,\n",
       "          3.95244546e-03, 2.45501287e-05, 3.66272870e-05],\n",
       "         [2.12430023e-05, 2.24919058e-05, 9.99905169e-01,\n",
       "          8.89279181e-06, 2.14655884e-05, 2.07982957e-05],\n",
       "         [1.41568482e-03, 1.41356082e-03, 4.61947098e-02,\n",
       "          5.85765811e-04, 9.50307131e-01, 8.31070356e-05],\n",
       "         ...,\n",
       "         [2.52125319e-05, 2.47348798e-05, 4.86886129e-05,\n",
       "          9.43157356e-05, 5.18215820e-05, 9.99755204e-01],\n",
       "         [2.85450369e-05, 1.79146882e-05, 9.99876201e-01,\n",
       "          3.45909502e-05, 1.95069006e-05, 2.32276507e-05],\n",
       "         [1.35123264e-05, 7.70384213e-06, 9.99815762e-01,\n",
       "          1.02109625e-04, 1.81704527e-05, 4.27714549e-05]]],\n",
       "\n",
       "\n",
       "       [[[1.56096648e-05, 7.49883475e-06, 2.55554914e-05,\n",
       "          9.99907672e-01, 2.49003060e-05, 1.87635887e-05],\n",
       "         [3.62631399e-05, 2.07042322e-05, 9.99916196e-01,\n",
       "          6.85686246e-06, 9.05804336e-06, 1.09042739e-05],\n",
       "         [1.73478620e-05, 1.47598330e-05, 9.64623177e-06,\n",
       "          9.99914885e-01, 1.67514663e-05, 2.66651623e-05],\n",
       "         ...,\n",
       "         [1.12724956e-05, 2.91266479e-05, 9.56113217e-06,\n",
       "          9.99915004e-01, 2.37885397e-05, 1.12504931e-05],\n",
       "         [1.89346028e-05, 1.44096557e-05, 1.73262088e-05,\n",
       "          1.77057227e-05, 9.99922395e-01, 9.18895239e-06],\n",
       "         [1.69035047e-05, 2.17193738e-05, 9.99912024e-01,\n",
       "          1.73349399e-05, 2.03463715e-05, 1.16878655e-05]],\n",
       "\n",
       "        [[2.42441893e-05, 1.14579452e-05, 1.62186334e-05,\n",
       "          2.93639023e-05, 1.38465548e-05, 9.99904811e-01],\n",
       "         [4.11649235e-05, 7.99709233e-06, 9.99907255e-01,\n",
       "          1.52362045e-05, 1.29173277e-05, 1.54414447e-05],\n",
       "         [3.52479983e-05, 1.14509603e-05, 1.94677850e-05,\n",
       "          9.99906719e-01, 2.02448573e-05, 6.93369657e-06],\n",
       "         ...,\n",
       "         [1.23939244e-05, 2.63187103e-05, 1.99287897e-05,\n",
       "          1.36853196e-05, 9.99905407e-01, 2.22113449e-05],\n",
       "         [1.82709191e-05, 1.61958160e-05, 1.10903056e-05,\n",
       "          1.96148176e-05, 9.99926507e-01, 8.32439400e-06],\n",
       "         [2.03258824e-05, 1.81318028e-05, 9.99915063e-01,\n",
       "          2.38609500e-05, 1.09665561e-05, 1.17566669e-05]],\n",
       "\n",
       "        [[2.08832789e-05, 2.45140400e-05, 8.23906157e-06,\n",
       "          9.99914229e-01, 1.85412355e-05, 1.35704177e-05],\n",
       "         [1.94901368e-05, 1.74582237e-05, 9.99904454e-01,\n",
       "          1.46608800e-05, 2.96777580e-05, 1.42159406e-05],\n",
       "         [1.27018429e-05, 3.84242740e-05, 1.80315692e-05,\n",
       "          9.99902725e-01, 1.29332766e-05, 1.52288703e-05],\n",
       "         ...,\n",
       "         [1.69617124e-05, 3.07802111e-05, 1.26179075e-05,\n",
       "          1.46888196e-05, 9.99911070e-01, 1.38240866e-05],\n",
       "         [1.92014268e-05, 1.56052411e-05, 2.15834007e-05,\n",
       "          1.01603800e-05, 9.99909401e-01, 2.40304507e-05],\n",
       "         [1.18293101e-05, 1.04655046e-05, 9.99908924e-01,\n",
       "          2.09680293e-05, 1.39086042e-05, 3.39949038e-05]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[2.68423464e-05, 2.28502322e-05, 1.98262278e-05,\n",
       "          8.04608222e-04, 9.99117792e-01, 8.15617386e-06],\n",
       "         [2.47771386e-05, 1.34233851e-05, 9.99640226e-01,\n",
       "          2.19948124e-05, 2.18211208e-05, 2.77804211e-04],\n",
       "         [9.62463673e-05, 9.85052902e-05, 9.98744011e-01,\n",
       "          4.93365165e-04, 5.42711932e-04, 2.52551399e-05],\n",
       "         ...,\n",
       "         [2.71156896e-05, 1.98516063e-05, 2.11898005e-05,\n",
       "          2.42294045e-05, 9.99883175e-01, 2.43415125e-05],\n",
       "         [1.93112064e-05, 2.14760657e-05, 1.42776407e-05,\n",
       "          2.56807543e-05, 9.99899983e-01, 1.92385633e-05],\n",
       "         [3.43208667e-05, 1.23059144e-05, 9.99740839e-01,\n",
       "          1.04711158e-04, 5.47044910e-05, 5.31850383e-05]],\n",
       "\n",
       "        [[3.15902289e-05, 3.30291223e-05, 3.43065476e-05,\n",
       "          9.99840200e-01, 3.25285364e-05, 2.83888076e-05],\n",
       "         [1.57026807e-05, 2.61776149e-05, 9.99900997e-01,\n",
       "          6.56145858e-06, 1.68494880e-05, 3.37478705e-05],\n",
       "         [9.99625564e-01, 4.20780852e-05, 8.35668761e-05,\n",
       "          3.98794655e-05, 1.05908141e-04, 1.02979713e-04],\n",
       "         ...,\n",
       "         [1.41023193e-05, 2.74204649e-05, 2.75596976e-05,\n",
       "          4.68317885e-05, 9.99872804e-01, 1.12901907e-05],\n",
       "         [2.87564471e-05, 9.01211752e-06, 1.49511034e-05,\n",
       "          1.97552145e-05, 9.99903679e-01, 2.39489600e-05],\n",
       "         [1.27488747e-05, 1.96406036e-05, 9.99437690e-01,\n",
       "          2.60313973e-05, 1.71382446e-04, 3.32379015e-04]],\n",
       "\n",
       "        [[1.79311261e-04, 2.07920093e-05, 3.83472070e-05,\n",
       "          2.31296290e-04, 5.94723970e-05, 9.99470770e-01],\n",
       "         [3.64482403e-05, 6.08974369e-06, 6.71411632e-04,\n",
       "          5.38886525e-05, 4.05889004e-05, 9.99191582e-01],\n",
       "         [3.84766608e-05, 1.23342033e-05, 7.01816753e-05,\n",
       "          9.99847531e-01, 2.04544049e-05, 1.10379187e-05],\n",
       "         ...,\n",
       "         [6.03939872e-04, 4.20952914e-04, 2.40365230e-03,\n",
       "          8.95155943e-04, 9.95453238e-01, 2.22950010e-04],\n",
       "         [1.08174281e-05, 2.26830598e-05, 2.56346539e-05,\n",
       "          1.71640422e-05, 9.99898195e-01, 2.54809856e-05],\n",
       "         [2.98446976e-05, 1.03479135e-04, 2.36026128e-04,\n",
       "          3.43293650e-05, 9.99544978e-01, 5.13591804e-05]]],\n",
       "\n",
       "\n",
       "       [[[5.41540794e-06, 3.23939603e-05, 1.02980994e-05,\n",
       "          9.99899745e-01, 3.01140826e-05, 2.20618676e-05],\n",
       "         [2.25473195e-05, 9.99913514e-01, 1.59547199e-05,\n",
       "          1.14453724e-05, 2.04583630e-05, 1.60551863e-05],\n",
       "         [2.29051802e-05, 9.99909461e-01, 1.16513111e-05,\n",
       "          2.51680613e-05, 1.77874463e-05, 1.30767003e-05],\n",
       "         ...,\n",
       "         [1.88942067e-05, 1.67766120e-05, 1.49238622e-05,\n",
       "          2.00027134e-05, 9.99912143e-01, 1.72572909e-05],\n",
       "         [1.03978673e-05, 3.50237824e-05, 1.49729894e-05,\n",
       "          1.50441192e-05, 9.99904990e-01, 1.95805915e-05],\n",
       "         [1.54048903e-05, 2.53545586e-05, 9.99912381e-01,\n",
       "          2.20329966e-05, 1.53785804e-05, 9.44902422e-06]],\n",
       "\n",
       "        [[9.99914110e-01, 1.80704519e-05, 2.60516535e-05,\n",
       "          8.92701792e-06, 7.67497113e-06, 2.51787715e-05],\n",
       "         [3.39541584e-05, 8.56647966e-06, 3.02118715e-05,\n",
       "          9.99900281e-01, 1.54637964e-05, 1.15089351e-05],\n",
       "         [1.70867424e-05, 9.99910712e-01, 1.03990315e-05,\n",
       "          1.67089747e-05, 1.57409813e-05, 2.93531921e-05],\n",
       "         ...,\n",
       "         [2.97764782e-05, 1.42338686e-05, 1.24638900e-05,\n",
       "          1.96314650e-05, 9.99902785e-01, 2.11170409e-05],\n",
       "         [2.04460230e-05, 1.09674875e-05, 1.71851134e-05,\n",
       "          1.67802209e-05, 1.85562531e-05, 9.99916077e-01],\n",
       "         [1.40161719e-05, 3.22510023e-05, 1.20037002e-05,\n",
       "          1.67596154e-05, 1.83815137e-05, 9.99906600e-01]],\n",
       "\n",
       "        [[9.99901056e-01, 6.42373925e-06, 2.38316134e-05,\n",
       "          2.88970768e-05, 1.43060461e-05, 2.55198684e-05],\n",
       "         [1.42506324e-05, 2.16143671e-05, 1.36145391e-05,\n",
       "          9.99910533e-01, 1.51869608e-05, 2.48013530e-05],\n",
       "         [1.82819786e-05, 9.99919474e-01, 8.12119106e-06,\n",
       "          1.55091984e-05, 2.08721031e-05, 1.77598558e-05],\n",
       "         ...,\n",
       "         [1.61577482e-05, 1.45633239e-05, 1.57515751e-05,\n",
       "          3.23762652e-05, 9.99896169e-01, 2.49557197e-05],\n",
       "         [1.46835810e-05, 2.15962064e-05, 1.60661293e-05,\n",
       "          1.20687764e-05, 1.45641388e-05, 9.99921024e-01],\n",
       "         [1.31981215e-05, 3.13036144e-05, 9.09873052e-06,\n",
       "          2.08383426e-05, 2.48108990e-05, 9.99900758e-01]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[1.28187705e-04, 1.72207365e-05, 1.56938331e-05,\n",
       "          2.23051757e-05, 2.38170614e-05, 9.99792755e-01],\n",
       "         [2.54011247e-05, 1.59931369e-05, 9.99888420e-01,\n",
       "          2.24071555e-05, 2.80428212e-05, 1.97173795e-05],\n",
       "         [2.11816514e-05, 9.99854565e-01, 1.76356407e-05,\n",
       "          2.85992865e-05, 5.94428275e-05, 1.84919918e-05],\n",
       "         ...,\n",
       "         [1.50596024e-05, 6.44810498e-05, 1.59355113e-05,\n",
       "          9.99877691e-01, 8.31723446e-06, 1.86129473e-05],\n",
       "         [9.97270405e-01, 1.73604349e-05, 7.51154730e-04,\n",
       "          1.49069750e-03, 1.62824290e-05, 4.54119407e-04],\n",
       "         [2.38441862e-05, 9.99882758e-01, 3.18521634e-05,\n",
       "          1.02317426e-05, 1.92618463e-05, 3.21592670e-05]],\n",
       "\n",
       "        [[2.53318576e-05, 2.20474321e-05, 1.69578707e-05,\n",
       "          2.61063688e-05, 9.99889135e-01, 2.03874661e-05],\n",
       "         [1.94513705e-05, 1.22130150e-05, 9.99903023e-01,\n",
       "          2.29955185e-05, 1.53296860e-05, 2.69846059e-05],\n",
       "         [2.74699414e-05, 3.03431880e-05, 1.26075465e-05,\n",
       "          9.99811769e-01, 1.07207568e-04, 1.05798244e-05],\n",
       "         ...,\n",
       "         [1.18257012e-05, 9.99634027e-01, 2.65883282e-05,\n",
       "          3.00006242e-04, 1.18972966e-05, 1.55932503e-05],\n",
       "         [9.99334455e-01, 2.00977083e-05, 3.95983225e-05,\n",
       "          5.76424878e-04, 1.32516725e-05, 1.62305078e-05],\n",
       "         [5.14441635e-05, 6.50933944e-05, 9.99719679e-01,\n",
       "          2.68949661e-05, 9.91895795e-05, 3.78773548e-05]],\n",
       "\n",
       "        [[4.84506600e-05, 1.34597067e-05, 1.59437768e-05,\n",
       "          1.92709267e-05, 1.27960229e-05, 9.99890089e-01],\n",
       "         [9.99904692e-01, 1.54093141e-05, 1.48884719e-05,\n",
       "          2.29466241e-05, 1.98457856e-05, 2.22357921e-05],\n",
       "         [9.99857783e-01, 4.98470617e-05, 1.67303951e-05,\n",
       "          3.89616471e-05, 2.65429262e-05, 1.02098566e-05],\n",
       "         ...,\n",
       "         [7.21501419e-06, 1.30755361e-05, 3.17669474e-05,\n",
       "          9.99905765e-01, 2.69233715e-05, 1.52866123e-05],\n",
       "         [1.51050393e-04, 1.10506080e-05, 4.94611450e-05,\n",
       "          2.43731774e-05, 1.98722119e-05, 9.99744177e-01],\n",
       "         [1.42391073e-05, 1.37952156e-05, 1.22700585e-05,\n",
       "          2.42157839e-05, 6.07080292e-05, 9.99874771e-01]]]],      dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key, subkey = jax.random.split(config.jax_key)\n",
    "config.jax_key = key\n",
    "\n",
    "sample = buffer.sample(buffer_list, subkey)\n",
    "sample = reshape_diffusion_setup(sample, subkey)\n",
    "\n",
    "\n",
    "result = sampling_model(key=config.jax_key, model=transformer, sample_eval=sample, config=config, nb_step=100)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[[4, 3, 1],\n",
       "        [4, 0, 2],\n",
       "        [3, 5, 3]],\n",
       "\n",
       "       [[2, 1, 5],\n",
       "        [5, 1, 3],\n",
       "        [0, 4, 5]],\n",
       "\n",
       "       [[4, 5, 4],\n",
       "        [2, 2, 0],\n",
       "        [2, 1, 4]],\n",
       "\n",
       "       [[5, 5, 1],\n",
       "        [2, 3, 3],\n",
       "        [3, 2, 1]],\n",
       "\n",
       "       [[0, 1, 0],\n",
       "        [0, 4, 4],\n",
       "        [2, 3, 1]],\n",
       "\n",
       "       [[2, 0, 3],\n",
       "        [4, 5, 0],\n",
       "        [5, 1, 0]]], dtype=int32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_batch  = 64\n",
    "\n",
    "jnp.argmax(sample[\"state_past\"], axis=-1).reshape((128, 8, 6, 3, 3))[index_batch, -1, :, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[[3, 4, 4],\n",
       "        [5, 0, 3],\n",
       "        [3, 2, 1]],\n",
       "\n",
       "       [[4, 5, 4],\n",
       "        [5, 1, 3],\n",
       "        [0, 4, 5]],\n",
       "\n",
       "       [[5, 5, 1],\n",
       "        [2, 2, 0],\n",
       "        [2, 1, 4]],\n",
       "\n",
       "       [[0, 1, 0],\n",
       "        [2, 3, 3],\n",
       "        [3, 2, 1]],\n",
       "\n",
       "       [[2, 1, 5],\n",
       "        [0, 4, 4],\n",
       "        [2, 3, 1]],\n",
       "\n",
       "       [[2, 0, 3],\n",
       "        [4, 5, 0],\n",
       "        [5, 1, 0]]], dtype=int32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jnp.argmax(result, axis=-1).reshape((128, 24, 6, 3, 3))[index_batch, 0, :, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[[4, 3, 3],\n",
       "        [4, 0, 2],\n",
       "        [3, 5, 5]],\n",
       "\n",
       "       [[2, 1, 1],\n",
       "        [5, 1, 2],\n",
       "        [0, 4, 0]],\n",
       "\n",
       "       [[4, 0, 4],\n",
       "        [5, 2, 1],\n",
       "        [4, 2, 2]],\n",
       "\n",
       "       [[0, 5, 1],\n",
       "        [0, 3, 3],\n",
       "        [3, 2, 1]],\n",
       "\n",
       "       [[0, 1, 0],\n",
       "        [0, 4, 4],\n",
       "        [2, 3, 1]],\n",
       "\n",
       "       [[2, 0, 3],\n",
       "        [4, 5, 3],\n",
       "        [5, 1, 5]]], dtype=int32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jnp.argmax(result, axis=-1).reshape((128, 24, 6, 3, 3))[index_batch, 1, :, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_past_state_with_with_random_policy(key, vmap_reset, step_jit_env, config):\n",
    "    \"\"\"\n",
    "    Generate past state with random policy\n",
    "\n",
    "    Args:\n",
    "        config: configuration object\n",
    "\n",
    "    Returns:\n",
    "        state_past: (batch_size, len_seq//4, 6, 3, 3)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    key1, key2 = jax.random.split(config.jax_key)\n",
    "\n",
    "    keys = jax.random.split(key1, config.batch_size)\n",
    "    state, timestep = vmap_reset(keys)\n",
    "\n",
    "    last_state = None\n",
    "    past_state = []\n",
    "\n",
    "    actions_all = jax.random.randint(\n",
    "        key=config.jax_key,\n",
    "        minval=env.action_spec.minimum,\n",
    "        maxval=env.action_spec.maximum,\n",
    "        shape=(config.batch_size, config.len_seq // 4, 3),\n",
    "    )\n",
    "\n",
    "    for i in range(config.len_seq // 4):\n",
    "\n",
    "        # apply random policy and retrieve state\n",
    "        action = actions_all[:, i, :]\n",
    "\n",
    "        state, timestep  = step_jit_env(state, action)\n",
    "        past_state.append(state.cube)\n",
    "\n",
    "    # concat all the past state to get the shape (batch_size, len_seq//4, 6, 3, 3) from a list of state of size (batch_size, 6, 3, 3) by creating the 1 axis\n",
    "    state_past = jnp.stack(past_state, axis=1)\n",
    "\n",
    "    return state_past, state, actions_all\n",
    "\n",
    "step_jit_env = jax.vmap(jit_step)\n",
    "\n",
    "state_past, state, actions_past = generate_past_state_with_with_random_policy(key, vmap_reset, step_jit_env, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 8, 6, 3, 3)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_past.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def apply_decision_diffuser_policy(key, state_past, decision_diffuser, inverse_rl_model, config, target_reward=0.5):\n",
    "    \"\"\"\n",
    "    1. Make a estimation of the targeted reward\n",
    "    2. Generate futur state with those targeted reward\n",
    "    3. Choose policy from that\n",
    "    \"\"\"\n",
    "    sample_eval = {\n",
    "        \"state_past\": jax.nn.one_hot(state_past, 6),\n",
    "    }\n",
    "\n",
    "    state_past = jnp.copy(state_past.reshape((state_past.shape[0], state_past.shape[1], -1)))\n",
    "    state_past = jax.nn.one_hot(state_past, num_classes=6)\n",
    "\n",
    "    state_future = sampling_model(key, decision_diffuser, sample_eval, nb_step=100, config=config, target_reward=target_reward)\n",
    "\n",
    "    # state_future is (batch_size, seq_len, dim_input_state / 6, 6)\n",
    "    state_to_act = jnp.concatenate([state_past, state_future], axis=1)\n",
    "    state_to_act_futur_t = state_to_act[:, (config.len_seq // 4 - 1):(-1), :, :]\n",
    "    state_to_act_futur_td1 = state_to_act[:, (config.len_seq // 4):, :, :]\n",
    "\n",
    "    # flatten the last 2 axis\n",
    "    state_to_act_futur_t = state_to_act_futur_t.reshape(\n",
    "        (state_to_act_futur_t.shape[0], state_to_act_futur_t.shape[1], -1)\n",
    "    )\n",
    "\n",
    "    state_to_act_futur_td1 = state_to_act_futur_td1.reshape(\n",
    "        (state_to_act_futur_td1.shape[0], state_to_act_futur_td1.shape[1], -1)\n",
    "    )\n",
    "\n",
    "    # now use reverse RL to compute the action TODO later\n",
    "    actions = inverse_rl_model(state_to_act_futur_t, state_to_act_futur_td1)\n",
    "\n",
    "    return actions\n",
    "\n",
    "actions_futur = apply_decision_diffuser_policy(config.jax_key, state_past, transformer, inverse_rl_model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from rubiktransformer.dataset import GOAL_OBSERVATION\n",
    "\n",
    "def gather_data_with_policy(state, state_past, actions_past, actions_futur, buffer, buffer_list, config):\n",
    "    \"\"\"\n",
    "    For loop with those policy and state\n",
    "\n",
    "    log performance compare to target\n",
    "\n",
    "    \"\"\"\n",
    "    state_futur_list = []\n",
    "\n",
    "    for i in range(config.len_seq - config.len_seq // 4):\n",
    "        actions_step = actions_futur[:, i, :]\n",
    "        actions_0 = jnp.argmax(actions_step[:, :6], axis=1)\n",
    "        actions_1 = jnp.argmax(actions_step[:, 6:], axis=1)\n",
    "\n",
    "        actions_full = jnp.stack([actions_0, jnp.zeros(config.batch_size), actions_1], axis=1)\n",
    "        \n",
    "        # transform to int type\n",
    "        actions_full = actions_full.astype(jnp.int32)\n",
    "    \n",
    "        # step \n",
    "        state, timestep  = step_jit_env(state, actions_full)\n",
    "\n",
    "        state_futur_list.append(state.cube)\n",
    "\n",
    "    # TODO SAVE DATA into batch format for later training\n",
    "    actions_0_all_futur = jnp.argmax(actions_futur[:, :, :6], axis=-1)\n",
    "    actions_1_all_futur = jnp.argmax(actions_futur[:, :, 6:], axis=-1)\n",
    "\n",
    "    action_all_futur = jnp.stack([actions_0_all_futur, jnp.zeros((config.batch_size, actions_0_all_futur.shape[1])), actions_1_all_futur], axis=-1)\n",
    "\n",
    "    action_all = jnp.concatenate([actions_past, action_all_futur], axis=1)\n",
    "    action_all = action_all.astype(jnp.int32)\n",
    "\n",
    "    state_futur = jnp.stack(state_futur_list, axis=1)\n",
    "\n",
    "    state_all = jnp.concatenate([state_past, state_futur], axis=1)\n",
    "\n",
    "    # compute reward \n",
    "    goal_observation = jnp.repeat(\n",
    "        GOAL_OBSERVATION[None, None, :, :, :], config.batch_size, axis=0\n",
    "    )\n",
    "    goal_observation = jnp.repeat(goal_observation, config.len_seq, axis=1)\n",
    "    reward = jnp.where(state_all != goal_observation, -1.0, 1.0)\n",
    "\n",
    "    reward = reward.mean(axis=[2, 3, 4])\n",
    "    reward = reward[:, -1] - reward[:, config.len_seq//4]\n",
    "    reward_whole = reward.max(axis=-1)\n",
    "\n",
    "\n",
    "    for idx_batch in range(config.batch_size):\n",
    "        buffer_list = buffer.add(\n",
    "            buffer_list,\n",
    "            {\n",
    "                \"action\": action_all[idx_batch],\n",
    "                \"reward\": reward[idx_batch],\n",
    "                \"state_histo\": state_all[idx_batch],\n",
    "            },\n",
    "        )\n",
    "\n",
    "    return buffer, buffer_list, reward, reward_whole\n",
    "\n",
    "buffer, buffer_list, reward_real, reward_whole = gather_data_with_policy(state, state_past, actions_past, actions_futur, buffer, buffer_list, config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "improve data buffer\n",
      "new target :  0.27734375\n",
      "trainign\n",
      "{'loss': Array(0.22688669, dtype=float32), 'loss_cross_entropy': Array(0.21618629, dtype=float32)}\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.18106192\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.12525319\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.09677011\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.06762695\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.053778753\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.04034424\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.034061007\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.033089533\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.0295656\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.019557107\n",
      "trainign\n",
      "{'loss': Array(0.13593225, dtype=float32), 'loss_cross_entropy': Array(0.12857307, dtype=float32)}\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.020773925\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.019935574\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.036298804\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.024370467\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.024627363\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.027070623\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.033355914\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.024490457\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.018755645\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.018058378\n",
      "trainign\n",
      "{'loss': Array(0.13932161, dtype=float32), 'loss_cross_entropy': Array(0.13209228, dtype=float32)}\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.023930809\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.024552211\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.026309673\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.023282152\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.01684941\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.02781128\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.03314754\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.025109649\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.028903205\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.020527992\n",
      "trainign\n",
      "{'loss': Array(0.13651428, dtype=float32), 'loss_cross_entropy': Array(0.12865952, dtype=float32)}\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.022995478\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.02697806\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.021012178\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.023671597\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.038600843\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.028125651\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.020573243\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.023018103\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.025108589\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.030349437\n",
      "trainign\n",
      "{'loss': Array(0.14956866, dtype=float32), 'loss_cross_entropy': Array(0.14178778, dtype=float32)}\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.027906202\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.024369767\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.03258419\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.026708761\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.027677298\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.040314343\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.02160393\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.012393402\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.012417766\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.012140596\n",
      "trainign\n",
      "{'loss': Array(0.1388127, dtype=float32), 'loss_cross_entropy': Array(0.13088436, dtype=float32)}\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.017644374\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.029221494\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.032695238\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.02734299\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.024811542\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.021809705\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.02696388\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.028238885\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.038280323\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.03765868\n",
      "trainign\n",
      "{'loss': Array(0.15608147, dtype=float32), 'loss_cross_entropy': Array(0.14762919, dtype=float32)}\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.030114062\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.022435505\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.018162197\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.030059107\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.02862909\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.037318017\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.041662484\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.032115966\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.031393632\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.01945839\n",
      "trainign\n",
      "{'loss': Array(0.14368314, dtype=float32), 'loss_cross_entropy': Array(0.13571535, dtype=float32)}\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.02549887\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.031701982\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.027859092\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.033171445\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.022951463\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.023773186\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.018686362\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.017734386\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.0142202005\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.025339268\n",
      "trainign\n",
      "{'loss': Array(0.1391248, dtype=float32), 'loss_cross_entropy': Array(0.13151859, dtype=float32)}\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.028873336\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.02890426\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.025013471\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.01684701\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.028678136\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.017811289\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.021781802\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.022464976\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.020202395\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.01212666\n",
      "trainign\n",
      "{'loss': Array(0.12763552, dtype=float32), 'loss_cross_entropy': Array(0.12058925, dtype=float32)}\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.0170587\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.01836731\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.027702171\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.03135687\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.02508237\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.022523824\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.015746865\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.013660471\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.02534875\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.03452044\n",
      "trainign\n",
      "{'loss': Array(0.13252573, dtype=float32), 'loss_cross_entropy': Array(0.12592676, dtype=float32)}\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.028110916\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.029535782\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.025184559\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.02561311\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.021487111\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.014649805\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.024541335\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.025146825\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.03152596\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.032979414\n",
      "trainign\n",
      "{'loss': Array(0.12682985, dtype=float32), 'loss_cross_entropy': Array(0.11943497, dtype=float32)}\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.030523272\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.032622747\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.032659754\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.022116914\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.01771355\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.026217883\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.028155237\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.021600764\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.025991354\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.026161186\n",
      "trainign\n",
      "{'loss': Array(0.14145036, dtype=float32), 'loss_cross_entropy': Array(0.13432421, dtype=float32)}\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.040134992\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.02527583\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.0337606\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.020063171\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.019580197\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.020496117\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.01632445\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.018723566\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.017318958\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.018063415\n",
      "trainign\n",
      "{'loss': Array(0.11063512, dtype=float32), 'loss_cross_entropy': Array(0.10383674, dtype=float32)}\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.03174583\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.03265532\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.029493172\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.025018577\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.018875029\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.015803255\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.017160885\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.02073322\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.022374712\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.02131467\n",
      "trainign\n",
      "{'loss': Array(0.12635313, dtype=float32), 'loss_cross_entropy': Array(0.1192477, dtype=float32)}\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.018759184\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.030068249\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.029791068\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.021261275\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.023506796\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.025786964\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.025624964\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.026990723\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.035052076\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.016079277\n",
      "trainign\n",
      "{'loss': Array(0.13414901, dtype=float32), 'loss_cross_entropy': Array(0.12703004, dtype=float32)}\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.026558155\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.029338103\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.0378172\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.038439848\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.030359969\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.027332762\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.03681453\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.035479024\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.04624067\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.039034687\n",
      "trainign\n",
      "{'loss': Array(0.11752231, dtype=float32), 'loss_cross_entropy': Array(0.11153153, dtype=float32)}\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.025304379\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.03435358\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.026870077\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.029204715\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.026176432\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.02003266\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.03273045\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.02967541\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.01859928\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.026082048\n",
      "trainign\n",
      "{'loss': Array(0.12960254, dtype=float32), 'loss_cross_entropy': Array(0.12331301, dtype=float32)}\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.025193803\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.024170976\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.025395673\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.02108904\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.029497067\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.039198767\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.036526464\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.032730825\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.03488393\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.02655655\n",
      "trainign\n",
      "{'loss': Array(0.11353194, dtype=float32), 'loss_cross_entropy': Array(0.10781373, dtype=float32)}\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.037439153\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.04128902\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.03800562\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.040270172\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.047189485\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.044283397\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.037911374\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.040367723\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.043910712\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.042933367\n",
      "trainign\n",
      "{'loss': Array(0.1267493, dtype=float32), 'loss_cross_entropy': Array(0.11951686, dtype=float32)}\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.031015294\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.023320146\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.028587157\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.034403533\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.027907785\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.018004818\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.017248936\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.02743234\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.020949967\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.023495816\n",
      "trainign\n",
      "{'loss': Array(0.12493058, dtype=float32), 'loss_cross_entropy': Array(0.11831363, dtype=float32)}\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.033159945\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.02873275\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.03548906\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.02628041\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.02789715\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.036373343\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.038296625\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.033615906\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.033879712\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.029816011\n",
      "trainign\n",
      "{'loss': Array(0.13756959, dtype=float32), 'loss_cross_entropy': Array(0.13076471, dtype=float32)}\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.027928839\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.02712993\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.035555705\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.03282415\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.036377355\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.03149886\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.032242484\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.030444158\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.03417463\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.042405598\n",
      "trainign\n",
      "{'loss': Array(0.12906095, dtype=float32), 'loss_cross_entropy': Array(0.12256303, dtype=float32)}\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.04261484\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.042864133\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.039227206\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.0382768\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.03158053\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.034019433\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.029162496\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.03194236\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.026966551\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.03822286\n",
      "trainign\n",
      "{'loss': Array(0.11703223, dtype=float32), 'loss_cross_entropy': Array(0.11062688, dtype=float32)}\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.039944768\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.03747817\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.043912698\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.040908895\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.031594492\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.032434978\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.030974435\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.028797405\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.024091989\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.022173308\n",
      "trainign\n",
      "{'loss': Array(0.10953794, dtype=float32), 'loss_cross_entropy': Array(0.10355373, dtype=float32)}\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n",
      "improve data buffer\n",
      "new target :  0.03539221\n",
      "trainign\n",
      "begin iter\n",
      "generate high reward value\n",
      "apply the strategy\n"
     ]
    }
   ],
   "source": [
    "def reward_hacking(reward):\n",
    "    \"\"\"\n",
    "    reward is an array of value of shape (batch_size, len_seq, 1) with value between -1 and 1\n",
    "    we want to apply to every element the funciton\n",
    "    f(x) = 0.1 * jnp.exp(4 * x)\n",
    "    \"\"\"\n",
    "\n",
    "    return 0.1 * jnp.exp(4.0 * reward)\n",
    "\n",
    "def improve_training_loop(buffer, buffer_list, nb_iter=10000):\n",
    "    \"\"\"\n",
    "    Relaunch the training loop with those new data incorporated into the buffer\n",
    "    \n",
    "    Full stuff here\n",
    "    Online transformer setup\n",
    "\n",
    "    1. We generate env setup \n",
    "    2. First random action in the different env\n",
    "    3. Use decision_diffuser to choose the action to do from here\n",
    "    4. Observe / apply policy  to retrieve data\n",
    "    5. Add the data into the buffer\n",
    "    6. Train model on those data\n",
    "\n",
    "    Remember to log the performance data to compare with other run / algorithms\n",
    "    \"\"\"\n",
    "    target_reward = 0.5\n",
    "    \n",
    "    for idx_step in range(nb_iter):\n",
    "\n",
    "        print(\"begin iter\")\n",
    "\n",
    "        key, subkey = jax.random.split(config.jax_key)\n",
    "        config.jax_key = key\n",
    "\n",
    "        print(\"generate high reward value\")\n",
    "        # first generate random state\n",
    "        state_past, state, actions_past = generate_past_state_with_with_random_policy(key, vmap_reset, step_jit_env, config)\n",
    "        \n",
    "        print(\"apply the strategy\")\n",
    "        # apply model to get some generation\n",
    "        actions_futur = apply_decision_diffuser_policy(config.jax_key, state_past, transformer, inverse_rl_model, config, target_reward)\n",
    "\n",
    "        print(\"improve data buffer\")\n",
    "        # update replay buffer dataset\n",
    "        buffer, buffer_list, reward_mean, reward_whole = gather_data_with_policy(state, state_past, actions_past, actions_futur, buffer, buffer_list, config)\n",
    "\n",
    "        diff_target = reward_mean - target_reward\n",
    "        target_reward = 1./2. * (target_reward + reward_mean.mean())\n",
    "        print(\"new target : \", target_reward)\n",
    "\n",
    "        wandb.log({\"reward_normalized\" : reward_hacking(reward_whole).mean(), \"target_reward_new\" : target_reward, \"diff_target_reward\": diff_target.mean()}, step=idx_step)\n",
    "\n",
    "        # now we can do the training loop\n",
    "        sample = buffer.sample(buffer_list, subkey)\n",
    "        sample = reshape_diffusion_setup(sample, subkey)\n",
    "\n",
    "        print(\"trainign\")\n",
    "\n",
    "        # we update the policy\n",
    "        train_step_transformer_rf(\n",
    "            transformer, optimizer_diffuser, metrics_train, sample\n",
    "        )\n",
    "\n",
    "        if idx_step % config.log_every_step == 0:\n",
    "            metrics_train_result = metrics_train.compute()\n",
    "            print(metrics_train_result)\n",
    "\n",
    "            wandb.log(metrics_train_result, step=idx_step)\n",
    "            metrics_train.reset()\n",
    "\n",
    "improve_training_loop(buffer, buffer_list, nb_iter=10000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
